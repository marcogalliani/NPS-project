---
title: "Conformal prediction: examples"
---

## Simple prediction intervals
### Gaussian data
```{r}
set.seed(2023)
n <- 100
grid_factor <- 1.25
alpha <- .1

y <- rnorm(n,mean=0, sd=1)
hist(y, col='lightblue')
```

Comparison between Fisher's prediction intervals and conformal ones

#### Parametric interval
**Fisher's prediction interval**

When $X_1,..., X_n$ and $X_{n+1}$ are sampled iid from a gaussian distribution $N(\mu, \sigma^2)$, with $\mu$ and $\sigma^2$ unknown, then we know that

$$
\frac{X_{n+1} - \bar{X}}{s \sqrt{1+\frac{1}{n}}} \sim t(n-1)
$$
Then, the prediction interval of level $1-\alpha$ for $X_{n+1}$ is the Fisher's prediction interval and has the form

$$
C_{1-\alpha}\left( \bar{X} - s \sqrt{1+\frac{1}{n}} t_{\alpha/2}(n-1), \bar{X} + s \sqrt{1+\frac{1}{n}} t_{\alpha/2}(n-1)\right)
$$
The prediction interval is exact in the sense that
$$
\mathbb{P} \left(X_{n+1} \in C_{1-\alpha} \right) = 1 - \alpha
$$

```{r}
wrapper_param <- function(grid_point, y){
  n  <- length(y)
  t0 <- (grid_point - mean(y))/sd(y)/sqrt(1+1/n)
  2*(1-pt(abs(t0), n-1))
}

# Create a grid of test points
n_grid <- 200
test_grid <- seq(-grid_factor*max(abs(y)),
                 +grid_factor*max(abs(y)),
                 length.out=n_grid)

pval_fun <- sapply(test_grid, wrapper_param, y)
plot(test_grid,pval_fun,type='l')
```

The prediction interval can be found by looking at the set of points corresponding to a p-value greater than $\alpha$
```{r}
index_in <- pval_fun>alpha
PI.param <- range(test_grid[index_in])
```

#### Conformal interval
```{r}
wrapper_full <- function(grid_point, y){
  #' y here is the vector of data points, while grid_point is the point of the grid that we're
  #' predicting
  
  aug_y <- c(grid_point,y)
  
  ncm <- numeric(length(aug_y))
  for (i in 1:length(aug_y)) {
    ncm[i] <- abs(aug_y[i] - mean(aug_y[-i]))/sd(aug_y[-i])/sqrt(1+1/length(aug_y[-i]))
  }
  sum(ncm>=ncm[1])/(length(aug_y))
}

# Create a grid of test points
n_grid <- 200
test_grid <- seq(-grid_factor*max(abs(y)),+grid_factor*max(abs(y)),length.out=n_grid)
pval_fun <- sapply(test_grid, wrapper_full, y)
plot(test_grid,pval_fun,type='l')
```
```{r}
index_in <- pval_fun>alpha
PI <- range(test_grid[index_in])

# Plot the prediction intervals
plot(test_grid,pval_fun,type='l')
abline(v=PI,col='blue') # Conformal prediction interval
abline(v=PI.param, col='darkorange') # Fisher's prediction interval
legend("topright", legend=c("Conformal", "Fisher's"), col=c("blue", "darkorange"),
       lty=1, cex=0.8)
```
The resulting intervals are both valid

### Heavy-tail distribution
```{r}
set.seed(2023)
n <- 100
y <- rt(n, df=2)
hist(y, col='lightblue')
```

Fisher interval
```{r}
# Create a grid of test points
n_grid <- 200
test_grid <- seq(-grid_factor*max(abs(y)),
                 +grid_factor*max(abs(y)),
                 length.out=n_grid)

pval_fun <- sapply(test_grid, wrapper_param, y)
plot(test_grid,pval_fun,type='l')

index_in <- pval_fun>alpha
PI.param <- range(test_grid[index_in])
PI.param
```

Conformal interval
```{r}
# Create a grid of test points
n_grid <- 200
test_grid <- seq(-grid_factor*max(abs(y)),+grid_factor*max(abs(y)),length.out=n_grid)
pval_fun <- sapply(test_grid, wrapper_full, y)
plot(test_grid,pval_fun,type='l')

index_in <- pval_fun>alpha
PI <- range(test_grid[index_in])
PI
```
Comparison
```{r}
# Plot the prediction intervals
plot(test_grid,pval_fun,type='l')
abline(v=PI,col='blue') # Conformal prediction interval
abline(v=PI.param, col='darkorange') # Fisher's prediction interval
legend("topright", legend=c("Conformal", "Fisher's"), col=c("blue", "darkorange"),
       lty=1, cex=0.8)
```

### Bimodal data
```{r}
n <- 100
grid_factor <- 1.25
alpha <- .1

set.seed(2023)
y <- c(rnorm(n/2,mean=-2.6),rnorm(n/2,mean=2.6))
hist(y, col='lightblue')
```
Using the same conformal measure as before
```{r}
# Create a grid of test points
n_grid <- 200
test_grid <- seq(-grid_factor*max(abs(y)),+grid_factor*max(abs(y)),length.out=n_grid)
pval_fun <- sapply(test_grid, wrapper_full, y)
plot(test_grid,pval_fun,type='l')

index_in <- pval_fun>alpha
PI.abs <- test_grid[as.logical(c(0,abs(diff(index_in))))]
PI.abs
```
Switching to euclidean distance from the $k$ nearest neighbours
```{r}
library(dbscan)

wrapper_knn <- function(grid_point,y){
  k_s <- 0.25
  aug_y <- c(grid_point,y)
  ncm <- kNNdist(matrix(aug_y),k_s*length(y))
  sum((ncm[-1]>=ncm[1]))/length(aug_y)
}

pval_fun <- sapply(test_grid, wrapper_knn, y)
plot(test_grid,pval_fun,type='l')

index_in <- pval_fun>alpha
PI.knn <- test_grid[as.logical(c(0,abs(diff(index_in))))]
PI.knn
```
Visualizing prediction intervals
```{r}
# Plot the prediction intervals
hist(y, col='lightblue')
abline(v=PI.abs,col='blue') # Conformal PI with "classical" ncm
abline(v=PI.knn, col='darkred') # Conformal PI with k-nn ncm
legend("topright", legend=c("PI abs", "PI k-nn"), col=c("blue", "darkred"),
       lty=1, cex=0.8)
```

The computed intervals remains valid but improves efficiency


## Conformal prediction bands for regression
```{r}
n <- 1e3

x <- runif(n,0,1)
epsilon <- 0.2 * (x^2) * rnorm(n,0,1) 
y <- x + epsilon


n.test <- 1e3
x.test <- runif(n.test,0,1)
y.test <- x.test + 0.2 * (x.test^2) * rnorm(n.test,0,1) 
```


```{r}
evaluate_predictions <- function(x.test, y.test, x.grid, lower, upper, view_plot=T){
  covered <- (y.test >= lower)*(y.test<=upper)
  coverage <- mean(covered)
  width <- mean(upper-lower)
  
  if(view_plot){
    idx.sort <- sort(x.test$x, index.return=TRUE)$ix
    plot(x.test$x[idx.sort], y.test[idx.sort], col='lightblue', main=paste0("Prediction interval, alpha=",alpha, ", coverage=", round(coverage,2), ", width=", round(width,2)),
     xlab="x test", ylab="y test")
    lines(x.test$x[idx.sort], lower[idx.sort], lty=3, col='black', lwd=2)
    lines(x.test$x[idx.sort], upper[idx.sort], lty=3, col='black', lwd=2)
  }
  
  out <- list("coverage"=coverage,
              "width"=width)
  return(out)
}
```

### Manual implementation
```{r}
#' Compute conformal prediction interval
reg_split_conformal <- function(x, y, x.test, alpha){
  n <- length(y)
  n.train <- floor(n/2)
  n.calib <- n-n.train
  
  # Split the data into training and calibrations sets
  idxs.train <- sample(1:n, size=n.train)
  
  x.train <- x[idxs.train]
  y.train <- y[idxs.train]
  x.calib <- x[-idxs.train]
  y.calib <- y[-idxs.train]
  
  data <- data.frame("x"=x.train, "y"=y.train)
  mod <- lm(y~x, data)
  ncs.calib <- abs(y.calib - predict(mod, data.frame("x"=x.calib)))
  ncs.quantile <- quantile(ncs.calib, prob=(1-alpha)*(n.calib+1)/n.calib)
  
  # Construct the prediction interval
  x.test <- data.frame("x"=x.test)
  y.hat <- predict(mod, x.test)
  lower_bound <- y.hat - ncs.quantile
  upper_bound <- y.hat + ncs.quantile
  
  out <- list("lower_bound"=lower_bound,
              "upper_bound"=upper_bound)
  
}

# Nominal significance level
alpha <- .1
pi.conformal <- reg_split_conformal(x, y, x.test, alpha)

performance <- evaluate_predictions(data.frame("x"=x.test), y.test, 
                                    lower=pi.conformal$lower_bound, 
                                    upper=pi.conformal$upper_bound)
```

### Conformal Inference package
```{r}
library(conformalInference)
```

```{r}
linear.fit <- lm(y ~ x, data=data.frame("x"=x, "y"=y))

# Define the design matrix of the model
linear.design.matrix <- model.matrix(y ~ x, data=data.frame("x"=x, "y"=y))

# And evaluate all predictors over a grid
pred_grid <- cbind(1,x.test)
colnames(pred_grid) <- colnames(linear.design.matrix)
```

Using full-conformal framework:
```{r}
c_preds <- conformal.pred(linear.design.matrix, 
                          y, 
                          pred_grid, 
                          alpha=0.05, 
                          verbose=F, 
                          train.fun=lm.funs(intercept = F)$train.fun, 
                          predict.fun=lm.funs(intercept = F)$predict.fun, 
                          num.grid.pts = 200)
```

A little note: 
In `train.fun=lm.funs(intercept = F)$train.fun, predict.fun=lm.funs(intercept = F)$predict.fun` we're passing the methods used to fit our model and to make prediction with it. If we specify `intercept=T` then we don't have to pass the intercept col in the provided design matrices, if `intercept=F` then we have to specify them. We just have to be consistent between design matrices and fit and predict methods!

```{r}
plot(x, y, xlim=range(x.test), cex =.5, col ="lightblue", main='Linear Regression')

lines(x.test, c_preds$pred, lwd=2, col ="black", lty=1)
matlines(x.test, cbind(c_preds$up,c_preds$lo), lwd=2, col="black",lty=2)
```

Using split-conformal framework (the one that we implemented manually): the only difference is that we use `conformal.pred.split`
```{r}
c_preds_split <- conformal.pred.split(
  linear.design.matrix, 
  y, 
  pred_grid, 
  alpha=0.05, 
  verbose=F, 
  train.fun=lm.funs(intercept = F)$train.fun, 
  predict.fun=lm.funs(intercept = F)$predict.fun
  )

plot(x, y, xlim=range(x.test), cex =.5, col ="lightblue", main='Linear Regression')

lines(x.test, c_preds_split$pred, lwd=2, col ="black", lty=1)
matlines(x.test, cbind(c_preds_split$up,c_preds_split$lo), lwd=2, col="black",lty=2)
```

##Prediction for various smoothers
```{r}
data("Prestige")
income.grid <- seq(range(Prestige$income)[1],range(Prestige$income)[2],by=100)
```


### Splines
```{r}
library(splines)

data("Prestige")

breaks <- c(quantile(Prestige$income, probs=c(0.2,0.4,0.6,0.8)), 15000)
B.splines.fit <- lm(prestige ~ bs(income, degree=3, knots=breaks), 
                    data = Prestige)

# Define the design matrix
B.splines.design.matrix <- with(Prestige,
                                bs(income, degree=3, knots=breaks))

# Predictors evaluated over a grid
pred_grid <- matrix(bs(income.grid, degree=3, knots=breaks), 
                    nrow=length(income.grid))
```

As before,
```{r}
c_preds_split <- conformal.pred.split(
  B.splines.design.matrix, 
  Prestige$prestige, 
  pred_grid, 
  alpha=0.05, 
  verbose=F, 
  train.fun=lm.funs(intercept = T)$train.fun, 
  predict.fun=lm.funs(intercept = T)$predict.fun
  )

with(Prestige,
     plot(income, prestige, 
          xlim=range(income.grid), cex =.5, col ="lightblue", 
          main='B-splines smoothing'))

lines(income.grid, c_preds_split$pred, lwd=2, col ="black", lty=1)
matlines(income.grid, cbind(c_preds_split$up,c_preds_split$lo), lwd=2, col="black",lty=2)
```

### Smoothing splines
```{r}
smoothing.splines.fit <- with(Prestige, smooth.spline(income, prestige))
opt <- smoothing.splines.fit$df

train_ss <- function(x, y, out=NULL){
  smooth.spline(x, y, df=opt)
}

predict_ss <- function(obj, new_x){
  predict(obj, new_x)$y
}
```

And let's reapply as before
```{r}
c_preds_split <- conformal.pred.split(
  Prestige$income, #Look at how the train and predict methods are defined
  Prestige$prestige, 
  income.grid, #Look at how the train and predict methods are defined
  alpha=0.05, 
  verbose=F, 
  train.fun=train_ss, 
  predict.fun=predict_ss
  )

with(Prestige,
     plot(income, prestige, 
          xlim=range(income.grid), cex =.5, col ="lightblue", 
          main='Smoothing-splines smoothing'))

lines(income.grid, c_preds_split$pred, lwd=2, col ="black", lty=1)
matlines(income.grid, cbind(c_preds_split$up,c_preds_split$lo), lwd=2, col="black",lty=2)
```

### GAMs
```{r}
library(mgcv)

gam.fit <- gam(prestige ~ s(education, bs='cr') + s(income, bs='cr'),
               data = Prestige)

# Evaluate the predictors on a grid
education.grid <- with(Prestige, seq(range(education)[1], range(education)[2], length.out=100))
income.grid <- with(Prestige, seq(range(income)[1], range(income)[2], length.out=100))

grid <- expand.grid(education.grid, income.grid)
names(grid) <- c('education', 'income')

pred <- predict(gam.fit, newdata=grid)
```

```{r}
train_gam <- function(x, y, out=NULL){
  colnames(x) <- c('var1','var2')
  train_data <- data.frame(y, x)
  model_gam <- gam(y ~ s(var1, bs='cr') + s(var2, bs='cr'), data=train_data)
}

predict_gam <- function(obj, new_x){
  new_x <- data.frame(new_x)
  colnames(new_x) <- c('var1', 'var2')
  predict.gam(obj, new_x)
}
```

```{r}
c_preds_split <- conformal.pred.split(
  cbind(Prestige$education,Prestige$income), 
  Prestige$prestige, 
  as.matrix(grid), 
  alpha=0.05,verbose=F, 
  train.fun=train_gam, 
  predict.fun=predict_gam)
```

Visualization
```{r}
with(Prestige,
     plot(income, prestige, 
          xlim=range(grid$income), cex =.5, col ="lightblue", 
          main='GAM smoothing'))

lines(grid$income,c_preds_split$pred, lwd=2, col ="black", lty=1)
matlines(grid$income, cbind(c_preds_split$up,c_preds_split$lo), lwd=2, col="black",lty=2)

with(Prestige,
     plot(education, prestige, 
          xlim=range(grid$education), cex =.5, col ="lightblue", 
          main='GAM smoothing'))

lines(grid$education,c_preds_split$pred, lwd=2, col ="black", lty=1)
matlines(grid$education, cbind(c_preds_split$up,c_preds_split$lo), lwd=2, col="black",lty=2)
```
## Multivariate response

### Manual implementation
```{r}
past_exams_fold = "../../past-exams"
milk_1 <- readRDS(paste(past_exams_fold,"2022-01-21/milk_samples_1.Rds",sep="/"))
```


To build the full conformal prediction interval let's follow these steps:

1. Build a grid of points on which we're gonna predict

grid: 20 equispaced points for each dimension
```{r}
data_predict <- milk_1[,-2]

n_grid <- 20
grid_factor <- 0.25
n <- nrow(data_predict)

range_x <- range(data_predict[,1])[2] - range(data_predict[,1])[1]
range_y <- range(data_predict[,2])[2] - range(data_predict[,2])[1]

test_grid_x <- seq(
  min(data_predict[,1]) - grid_factor*range_x,
  max(data_predict[,1]) + grid_factor*range_x,
  length.out = n_grid
)

test_grid_y <- seq(
  min(data_predict[,2]) - grid_factor*range_y,
  max(data_predict[,2]) + grid_factor*range_y,
  length.out = n_grid
)

xy_surface <- expand.grid(test_grid_x, test_grid_y)
colnames(xy_surface) <- colnames(data_predict)
```

2. Then we define a function to compute the p-value function of a new test point by ranking the non-conformity scores of the new dataset obtained by adding the new test point
```{r}
wrapper_multi_conf <- function(test_point){
  new_data <- rbind(test_point, data_predict)
  
  #computation of the non-conformity scores
  new_median <- depthMedian(new_data, depth_params = list(method='Tukey'))
  non_conf_scores <- rowSums(t(t(new_data)-new_median)^2)
  
  #compute the p-value for the new observation
  p_val <- sum(non_conf_scores[-1] >= non_conf_scores[1])/(n+1)
  
  return(p_val)
}
```

3. Compute the p-val for all the points in the grid
```{r}
library(pbapply)

pval_surf <- pbapply(xy_surface,1,wrapper_multi_conf)
```

4. Compute the prediction interval by thresholding the p-val function
```{r}
alpha <- 0.1

prediction_set <- xy_surface[pval_surf>alpha,]
```


Visualization
```{r}
library(ggplot2)

data_plot <- cbind(pval_surf,xy_surface)
poly_points <- prediction_set[chull(prediction_set),]

ggplot() +
  geom_tile(data=data_plot, aes(kappa_casein,Native_pH,fill=pval_surf)) +
  geom_point(data=data.frame(data_predict), aes(kappa_casein,Native_pH)) + 
  geom_polygon(data=poly_points, aes(kappa_casein,Native_pH),
               color='red', size=1, alpha=0.01)
```

## conformalInference.multi
### Tutorial
The `conformalInference.multi` package plays the multivariate counterpart of `conformalInference`.

```{r, message=FALSE, warning=FALSE}
#install.packages('conformalInference.multi')
library(conformalInference.multi)
```

Let's generate some multivariate data

```{r}
n=25
p=4
q=2

mu=rep(0,p)
x = mvtnorm::rmvnorm(n, mu)
beta<-sapply(1:q, function(k) c(mvtnorm::rmvnorm(1,mu)))
y = x%*%beta + t(mvtnorm::rmvnorm(q,1:(n)))
x0=x[n,]
y0=y[n,]

n0<-nrow(y0)
q<-ncol(y)
```

Let's declare the fitting and prediction function

```{r}
fun=mean_multi()

```

And then run the usual full conformal algorithm

```{r}
final.full <- conformal.multidim.full(x, y, x0, 
                                      fun$train.fun,
                                      fun$predict.fun, 
                                      score="l2",
                                      num.grid.pts.dim=100, grid.factor=1.25,
                                      verbose=FALSE)

plot_multidim(final.full)
```


I can customise the ncm, of course
```{r}
final.full <- conformal.multidim.full(x, y, x0, fun$train.fun,
                                      fun$predict.fun, score="max",
                                      num.grid.pts.dim=100, grid.factor=1.25,
                                      verbose=FALSE)

plot_multidim(final.full)
```


